{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18b89326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "from PIL import Image as im"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd243a2",
   "metadata": {},
   "source": [
    "##### 0, Define the hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae4b874e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"TODO: change hyper parameters\"\n",
    "n_epochs = 15 #Initially 3. \n",
    "batch_size_train = 64 \n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.1 #Changed manually with different optimization functions\n",
    "momentum = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f27185",
   "metadata": {},
   "source": [
    "##### 1, Loading training and testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de967eba",
   "metadata": {},
   "source": [
    "#### Organizing images\n",
    "\n",
    "First, we need to create a directory structure to hold our images.\n",
    "The directory <em>training2</em> will hold 10 directories, corresponding to class labels.\n",
    "\n",
    "This is so we can load the images using torchvision.datasets.ImageFolder, which expects the format described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8660d179-063a-489e-9a3e-5ba78f7eacec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#Make training2 directory\n",
    "if not os.path.exists('training2'):\n",
    "    os.mkdir(\"training2\")\n",
    "#Create the subdirectories corresponding to class labels\n",
    "for i in range(10):\n",
    "    if not os.path.exists(\"training2/{}\".format(i+1)):\n",
    "        os.mkdir(\"training2/{}\".format(i + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59cf5a99-0b08-45ca-ae9a-4496cc373f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "#Iterate through labels.txt, copying images from training into training2 based on their class label\n",
    "file = open(\"labels.txt\",\"r\")\n",
    "for line in file.readlines():\n",
    "    text = line.strip().split(\"\\t\")\n",
    "    shutil.copy(\"training/\" + text[0], \"training2/{0}/\".format(text[1]))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f9b6c015-2a68-49c4-8174-ec30e12bb4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Grayscale(),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))\n",
    "                               ])\n",
    "dataset = datasets.ImageFolder('training2',transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c7619435-b494-497f-ab8f-f8a767c6a94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the dataset into a testing and training set\n",
    "trainset,testset = random_split(dataset,[600,260])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6c2bfb-9a6f-438c-9ef5-6d0110ed1dbc",
   "metadata": {},
   "source": [
    "Passing the dataset to a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6075ae14-6928-4e55-86d4-b445b3a58bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=20, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=20, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a38b598",
   "metadata": {},
   "source": [
    "##### 1.3 First, check the documentation of dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "70088afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainloader = torch.utils.data.DataLoader(trainset0, batch_size=batch_size_train, shuffle=True)\n",
    "# testloader = torch.utils.data.DataLoader(testset0, batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc34ab73-1341-4a3f-adf2-434c480206bc",
   "metadata": {},
   "source": [
    "1.4 Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8e449eb2-5da5-4f0a-9a63-f19f4b1781c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 20 is out of bounds for dimension 0 with size 20",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [50]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m10\u001b[39m, index)\n\u001b[1;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39msqueeze(), cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgray_r\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 20 is out of bounds for dimension 0 with size 20"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dataiter = iter(trainloader)\n",
    "# images, labels = dataiter.next()\n",
    "\n",
    "# figure = plt.figure()\n",
    "# num_of_images = 60\n",
    "# for index in range(1, num_of_images + 1):\n",
    "#     plt.subplot(6, 10, index)\n",
    "#     plt.axis('off')\n",
    "#     plt.imshow(images[index].numpy().squeeze(), cmap='gray_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcda99c",
   "metadata": {},
   "source": [
    "##### 2 Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0ce69131",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a503bc0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Read the documentation of torchvision.models to try more cnn models'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Read the documentation of torchvision.models to try more cnn models\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4be229a",
   "metadata": {},
   "source": [
    "##### 3 Write the training function and the testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f4b55b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, device):\n",
    "    # evaluation, freeze \n",
    "    model.eval()\n",
    "    total_num = 0\n",
    "    total_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for _, (data, target) in enumerate(test_loader):\n",
    "            \n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            predict_one_hot = model(data)\n",
    "            \n",
    "            _, predict_label = torch.max(predict_one_hot, 1)\n",
    "            \n",
    "            total_correct += (predict_label == target).sum().item()\n",
    "            total_num += target.size(0)\n",
    "        \n",
    "    return (total_correct / total_num)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "681307a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, test_loader, num_epoch, learning_rate, momentum, device):\n",
    "    train_losses = []\n",
    "    \n",
    "    # 1, define optimizer\n",
    "    \n",
    "    \"TODO: try different optimizer\"\n",
    "    \n",
    "    \n",
    "    #optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "    #                  momentum=momentum)\n",
    "    \n",
    "    #The Adam algorithm should be more expensive to train, but may result in a higher accuracy\n",
    "    optimizer = optim.Adam(network.parameters()) #Using default learning rate of 1e-3\n",
    "    #optimizer = optim.AdamW(network.parameters())\n",
    "    \n",
    "    for epoch in tqdm(range(num_epoch)):\n",
    "        # train the model\n",
    "        model.train()\n",
    "        \n",
    "        for i, (data, target) in enumerate(train_loader):\n",
    "            \n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 2, forward\n",
    "            output = network(data)\n",
    "            \n",
    "            \n",
    "            # 3, calculate the loss\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \"TODO: try use cross entropy loss instead \"\n",
    "            \n",
    "            \n",
    "            #loss = F.nll_loss(output, target)\n",
    "            \n",
    "            loss = F.cross_entropy(output, target)\n",
    "            \n",
    "            # 4, backward\n",
    "            loss.backward()\n",
    "            \n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "        # evaluate the accuracy on test data for each epoch\n",
    "        accuracy = test(model, test_loader, device)\n",
    "        print('accuracy', accuracy)\n",
    "        \n",
    "    # 5, save model\n",
    "    \n",
    "    \"TODO: change the number of epochs save the model with the best prediction accuracy\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c2e854b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/15 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 320]' is invalid for input of size 1642800",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [49]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# use cpu if you do not have gpu installed in your computer\u001b[39;00m\n\u001b[1;32m      3\u001b[0m network \u001b[38;5;241m=\u001b[39m Net()\u001b[38;5;241m.\u001b[39mto(device0)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtestloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice0\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [48]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, test_loader, num_epoch, learning_rate, momentum, device)\u001b[0m\n\u001b[1;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# 2, forward\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# 3, calculate the loss\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTODO: try use cross entropy loss instead \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs101aFinal/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [45]\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(F\u001b[38;5;241m.\u001b[39mmax_pool2d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x), \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     12\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(F\u001b[38;5;241m.\u001b[39mmax_pool2d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2_drop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)), \u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m---> 13\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m320\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n\u001b[1;32m     15\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 320]' is invalid for input of size 1642800"
     ]
    }
   ],
   "source": [
    "device0 = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# use cpu if you do not have gpu installed in your computer\n",
    "network = Net().to(device0)\n",
    "train(model=network, train_loader=trainloader, test_loader=testloader, num_epoch=n_epochs, learning_rate=learning_rate, momentum=momentum, device=device0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a06b3d0",
   "metadata": {},
   "source": [
    "#### 4 Calculate the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f49b2c-40da-41ba-a489-7add5a8cdf40",
   "metadata": {},
   "source": [
    "0.98625 accuracy using the provided optimizer (SGD), loss function (SGD), and epochs (3).\n",
    "\n",
    "0.9895 accuracy using Adam optimization algorithm, cross entropy loss function, and 3 epcohs.\n",
    "\n",
    "0.99005 accuracy using Adam optimization algorithm, cross entropy loss function, and 5 epochs.\n",
    "\n",
    "0.98985 accuracy using AdamW optimization algorithm, cross entropy loss function, and 5 epochs.\n",
    "\n",
    "0.991325 accuracy using Adam optimization algorithm, cross entropy, and 15 epochs. \n",
    "\n",
    "Increasing the number of epochs increased the accuracy overall.\n",
    "\n",
    "The Adam or AdamW optimization algorithms seem to be the most accurate than SGD.\n",
    "\n",
    "I am running this on a virtual machine with 3gb of RAM, 1 cpu core, and no gpu, so training this takes a very long time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f8286f-8c00-4d3d-935f-a5cee15544ba",
   "metadata": {},
   "source": [
    "### Saving the model trained with Adam, cross entropy, 15 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73265995-1c5e-45fe-9861-4d1e68e79a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(network, './MNIST_model.pt') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c76556-51f4-456e-9145-0527a1dd651a",
   "metadata": {},
   "source": [
    "# 5 - Using a model without convolution\n",
    "\n",
    "Of course, a CNN is better suited for image recognition, but I want to compare it to a sequential model without convolution to see how it performs.\n",
    "\n",
    "Source: https://towardsdatascience.com/handwritten-digit-mnist-pytorch-977b5338e627"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd622986-8cf3-439b-8f23-1a9df1e8ea09",
   "metadata": {},
   "source": [
    "### Building the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001d0bf8-6ddb-40da-84f6-2f24bf06a67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784 #28*28 pixels\n",
    "hidden_sizes = [128, 64] #128 neurons in first hidden layer, 64 in second\n",
    "output_size = 10 #10 digits, 10 outputs\n",
    "\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(), \n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda06f1f-c02b-48c9-9970-748f6c8f5596",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d636835-ca56-420a-b135-e871aa0ba26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss() #nll loss function, same as initial above\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "logps = model(images) #log probabilities\n",
    "loss = criterion(logps, labels) #calculate the NLL loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b0d274-adc4-4f8e-b790-c39e2d71eb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.003, momentum=0.9) #SGD optimizer, same as the initial optimizer above\n",
    "epochs = 5\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "    \n",
    "        # Training pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        #This is where the model learns by backpropagating\n",
    "        loss.backward()\n",
    "        \n",
    "        #And optimizes its weights here\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(trainloader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2347c5-f290-4a8c-a8dd-85d7a112d753",
   "metadata": {},
   "source": [
    "### Testing this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333a6f01-883a-400b-ab88-75789339f45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_count, all_count = 0, 0\n",
    "for images,labels in testloader:\n",
    "  for i in range(len(labels)):\n",
    "    img = images[i].view(1, 784)\n",
    "    with torch.no_grad():\n",
    "        logps = model(img)\n",
    "\n",
    "    \n",
    "    ps = torch.exp(logps)\n",
    "    probab = list(ps.numpy()[0])\n",
    "    pred_label = probab.index(max(probab))\n",
    "    true_label = labels.numpy()[i]\n",
    "    if(true_label == pred_label):\n",
    "      correct_count += 1\n",
    "    all_count += 1\n",
    "\n",
    "print(\"Number Of Images Tested =\", all_count)\n",
    "print(\"\\nModel Accuracy =\", (correct_count/all_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcb0ec0-7808-4723-98b5-6b6222149616",
   "metadata": {},
   "source": [
    "0.984125 accuracy with 5 epochs, SGD optimization, and nll loss function.\n",
    "\n",
    "Using the same optimization algorithm and loss function above in the CNN, we achieved 0.98625 accuracy in just 3 epochs.\n",
    "\n",
    "CNN therefore appears better suited to image recognition, although using this model is not far off. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11800328-5151-4918-8a6a-ec49884ba15d",
   "metadata": {},
   "source": [
    "# In Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f15a6c0-471b-4263-b499-8bea4431d92a",
   "metadata": {},
   "source": [
    "For the CNN, the Adam algorithm works better than the SGD.\n",
    "\n",
    "Cross entropy worked better than nll as a loss function.\n",
    "\n",
    "More epochs are better. I tested 3, 5, and 15. \n",
    "\n",
    "After 5 epochs, the accuracy went down slightly before rising again. \n",
    "\n",
    "Using convolution, we can achieve higher image classification accuracy.\n",
    "\n",
    "\n",
    "Overall, I felt that I was limited by the performance of my computer. As mentioned above, I am running this on a virtual machine with 3gb RAM, 1 CPU core, and no CUDA/GPU. I imagine that if I had a better computer, I could more easily test different hyper-parameters and increase the number of epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcc36d7-1fcc-4acf-895c-5b4edc14278b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
